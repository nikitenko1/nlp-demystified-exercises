{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F50G99nH112P"
   },
   "source": [
    "# Natural Language Processing Demystified | Simple Vectorization\n",
    "https://nlpdemystified.org<br>\n",
    "https://github.com/futuremojo/nlp-demystified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9x6fL6L3zsb"
   },
   "source": [
    "### spaCy upgrade and package installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88uW0zDh4BkP"
   },
   "source": [
    "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy and download a statisical language model.\n",
    "<br><br>\n",
    "**IMPORTANT**<br>\n",
    "If you're running this in the cloud rather than using a local Jupyter server on your machine, then the notebook will **timeout** after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical packages.\n",
    "<br><br>\n",
    "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "THBGyQba4Bcm"
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy==3.*\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t81VT9JboTzt"
   },
   "source": [
    "# Basic Bag-of-Words (BOW)\n",
    "\n",
    "Course module for this demo: https://www.nlpdemystified.org/course/basic-bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u_EAof8njfHz"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1IVdG29wyJ7"
   },
   "source": [
    "## Plain frequency BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2fwfWQDVyJpY"
   },
   "outputs": [],
   "source": [
    "# A corpus of sentences.\n",
    "corpus = [\n",
    "  \"Red Bull drops hint on F1 engine.\",\n",
    "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
    "  \"Hamilton eyes record eighth F1 title.\",\n",
    "  \"Aston Martin announces sponsor.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILvS020Zzm6F"
   },
   "source": [
    "We want to build a basic bag-of-words (BOW) representation of our corpus. Based on what you now know from the lesson, you can probably do this from scratch using dictionaries and lists (and maybe that's a good exercise). Fortunately, there are robust libraries which make it easy.\n",
    "\n",
    "We can use the scikit-learn **CountVectorizer** which takes a collection of text documents and creates a matrix of token counts:<br>\n",
    "https://scikit-learn.org/stable/index.html<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IRhJPxbHwuj_"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
       "       'second document', 'the first', 'the second', 'the third',\n",
       "       'third one', 'this document', 'this is', 'this the'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "vectorizer2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAphZMVPBX9P"
   },
   "source": [
    "The *fit_transform* method does two things:\n",
    "1. It learns a vocabulary dictionary from the corpus.\n",
    "2. It returns a matrix where each row represents a document and each column represents a token (i.e. term).<br>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-5wi4_C7BAWv"
   },
   "outputs": [],
   "source": [
    "bow = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3Bp1XNcF1FQ"
   },
   "source": [
    "We can take a look at the features and vocabulary dictionary. Notice the **CountVectorizer** took care of tokenization for us. It also removed punctuation and lower-cased everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fQbqvLgVF8B7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View features (tokens).\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# View vocabulary dictionary.\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dmNUkZeExam"
   },
   "source": [
    "Specifically, the **CountVectorizer** generates a sparse matrix using an efficient, compressed representation. The sparse matrix object includes a number of useful methods:\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Lug2-xnAExsb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bywJ0XnGKPQ"
   },
   "source": [
    "If we look at the raw structure, we'll see tuples where the first element represents the document, and the second element represents a token ID. It's then followed by a count of that token. So in the second document (index 1), token 8 (\"f1\") occurs twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "At6Gt4bsEx2D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv1N1Io2EyAb"
   },
   "source": [
    "Before we explore further, we want to make a few modifications.\n",
    "1. What if we want to use another tokenizer like spaCy's?\n",
    "2. Instead of frequency, what if we want to have a binary BOW?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRgIHkzUVJtk"
   },
   "source": [
    "## Binary BOW with custom tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tof1PBgqEy1D"
   },
   "source": [
    "**CountVectorizer** supports using a custom tokenizer. For every document, it will call your tokenizer and expect a list of tokens returned. We'll create a simple callback below which has spaCy tokenize and filter tokens, and then return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AcCLawrWEzC7"
   },
   "outputs": [],
   "source": [
    "# As usual, we start by importing spaCy and loading a statistical model.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a tokenizer callback using spaCy under the hood. Here, we tokenize\n",
    "# the passed-in text and return the tokens, filtering out punctuation.\n",
    "def spacy_tokenizer(doc):\n",
    "  return [t.text for t in nlp(doc) if not t.is_punct]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drEe1Lv_OScv"
   },
   "source": [
    "This time, we instantiate **CountVectorizer** with our custom tokenizer (*spacy_tokenizer*), turn off case-folding, and also set the *binary* parameter to *True* so we simply get 1s and 0s marking token presence rather than token frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1YREyWzaA-rT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\38067\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True)\n",
    "bow = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jDKQkZUOysa"
   },
   "source": [
    "Looking at the resulting feature names and vocabulary dictionary, we can see our *spacy_tokenizer* being used. If you're not convinced, you can remove the punctuation filtering in our tokenizer and rerun the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4x6RBqTGq302"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And' 'Is' 'This' 'document' 'first' 'is' 'one' 'second' 'the' 'third'\n",
      " 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'This': 2,\n",
       " 'is': 5,\n",
       " 'the': 8,\n",
       " 'first': 4,\n",
       " 'document': 3,\n",
       " 'second': 7,\n",
       " 'And': 0,\n",
       " 'this': 10,\n",
       " 'third': 9,\n",
       " 'one': 6,\n",
       " 'Is': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFpQbdA-R3FI"
   },
   "source": [
    "To get a dense array representation of our sparse matrix, use *toarray*.<br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html#scipy.sparse.csr_matrix.toarray\n",
    "\n",
    "We can also index and slice into the sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2yGr36aP9GCr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dense representation like we saw in the slides.\n",
      "[[0 0 1 1 1 1 0 0 1 0 0]\n",
      " [0 0 1 1 0 1 0 1 1 0 0]\n",
      " [1 0 0 0 0 1 1 0 1 1 1]\n",
      " [0 1 0 1 1 0 0 0 1 0 1]]\n",
      "\n",
      "Indexing and slicing.\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 3)\t1\n",
      "\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "print('A dense representation like we saw in the slides.')\n",
    "print(bow.toarray())\n",
    "print()\n",
    "print('Indexing and slicing.')\n",
    "print(bow[0])\n",
    "print()\n",
    "print(bow[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XF0NVhdEUR1r"
   },
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leI1VuDVVP4W"
   },
   "source": [
    "Writing your own cosine similarity function is straight-forward using numpy (left as an exercise). There are multiple ways to calculate it using scipy.\n",
    "<br><br>\n",
    "One way is using the **spatial** package, which is a collection of spatial algorithms and data structures. It has a method to calculate cosine *distance*. To get the cosine *similarity*, we have to substract the distance from 1.<br>\n",
    "https://docs.scipy.org/doc/scipy/reference/spatial.html<br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kOQQ50IgXQfH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
      "Doc 1 vs Doc 2: 0.8\n",
      "Doc 1 vs Doc 3: 0.3651483716701107\n",
      "Doc 1 vs Doc 4: 0.6\n"
     ]
    }
   ],
   "source": [
    "# The cosine method expects array_like inputs, so we need to generate\n",
    "# arrays from our sparse matrix.\n",
    "doc1_vs_doc2 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[1].toarray()[0])\n",
    "doc1_vs_doc3 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[2].toarray()[0])\n",
    "doc1_vs_doc4 = 1 - spatial.distance.cosine(bow[0].toarray()[0], bow[3].toarray()[0])\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "print(f\"Doc 1 vs Doc 2: {doc1_vs_doc2}\")\n",
    "print(f\"Doc 1 vs Doc 3: {doc1_vs_doc3}\")\n",
    "print(f\"Doc 1 vs Doc 4: {doc1_vs_doc4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SRDwr2gYD04"
   },
   "source": [
    "Another approach is using scikit-learn's *cosine_similarity* which computes the metric between multiple vectors. Here, we pass it our BOW and get a matrix of cosine similarities between each document.<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WwwP8-jtchSI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.8        0.36514837 0.6       ]\n",
      " [0.8        1.         0.36514837 0.4       ]\n",
      " [0.36514837 0.36514837 1.         0.36514837]\n",
      " [0.6        0.4        0.36514837 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity can take either array-likes or sparse matrices.\n",
    "print(cosine_similarity(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I96W6qDVdDnY"
   },
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3E_hN5Ddyae"
   },
   "source": [
    "**CountVectorizer** includes an *ngram_range* parameter to generate different n-grams. n_gram range is specified using a minimum and maximum range. By default, n_gram range is set to (1, 1) which generates unigrams. Setting it to (1, 2) generates both unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OZooyyRleHXe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And' 'And this' 'Is' 'Is this' 'This' 'This document' 'This is'\n",
      " 'document' 'document is' 'first' 'first document' 'is' 'is the' 'one'\n",
      " 'second' 'second document' 'the' 'the first' 'the second' 'the third'\n",
      " 'third' 'third one' 'this' 'this is' 'this the']\n",
      "Number of features: 25\n",
      "{'This': 4, 'is': 11, 'the': 16, 'first': 9, 'document': 7, 'This is': 6, 'is the': 12, 'the first': 17, 'first document': 10, 'second': 14, 'This document': 5, 'document is': 8, 'the second': 18, 'second document': 15, 'And': 0, 'this': 22, 'third': 20, 'one': 13, 'And this': 1, 'this is': 23, 'the third': 19, 'third one': 21, 'Is': 2, 'Is this': 3, 'this the': 24}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(1,2))\n",
    "bigrams = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('Number of features: {}'.format(len(vectorizer.get_feature_names_out())))\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Hvtmi3negc0G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And this' 'Is this' 'This document' 'This is' 'document is'\n",
      " 'first document' 'is the' 'second document' 'the first' 'the second'\n",
      " 'the third' 'third one' 'this is' 'this the']\n",
      "{'This is': 3, 'is the': 6, 'the first': 8, 'first document': 5, 'This document': 2, 'document is': 4, 'the second': 9, 'second document': 7, 'And this': 0, 'this is': 12, 'the third': 10, 'third one': 11, 'Is this': 1, 'this the': 13}\n"
     ]
    }
   ],
   "source": [
    "# Setting n_gram range to (2, 2) generates only bigrams.\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(2,2))\n",
    "bigrams = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7e40ZAKhQmm"
   },
   "source": [
    "## Basic Bag-of-Words Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "dbdMO0bZjROn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      "['about' 'aerial' 'against' 'al' 'are' 'aren' 'as' 'assad' 'bashar'\n",
      " 'birdview' 'canon' 'cellphones' 'cmos' 'contours' 'danger' 'days'\n",
      " 'digital' 'during' 'early' 'enabled' 'enthusiastic' 'features' 'find'\n",
      " 'from' 'gps' 'great' 'ground' 'heaps' 'identify' 'if' 'image' 'in' 'is'\n",
      " 'it' 'lake' 'land' 'leader' 'level' 'much' 'neighbourhood' 'of' 'or'\n",
      " 'order' 'paths' 'pay' 'photograph' 'photographs' 'photography' 'points'\n",
      " 'president' 'pretty' 'price' 'river' 'rubbish' 'sensor' 'slrs' 'specific'\n",
      " 'strikes' 'students' 'study' 'such' 'syria' 'syrian' 'take' 'taking'\n",
      " 'technology' 'teenagers' 'tells' 'terrestrial' 'that' 'the' 'their' 'to'\n",
      " 'undisputed' 'us' 'use' 'visible' 'was' 'way' 'will']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Create a spacy_tokenizer callback which takes a string and returns\n",
    "# a list of tokens (each token's text) with punctuation filtered out.\n",
    "#\n",
    "corpus = [\n",
    "  \"Students use their GPS-enabled cellphones to take birdview photographs of a land in order to find specific danger points such as rubbish heaps.\",\n",
    "  \"Teenagers are enthusiastic about taking aerial photograph in order to study their neighbourhood.\",\n",
    "  \"Aerial photography is a great way to identify terrestrial features that aren’t visible from the ground level, such as lake contours or river paths.\",\n",
    "  \"During the early days of digital SLRs, Canon was pretty much the undisputed leader in CMOS image sensor technology.\",\n",
    "  \"Syrian President Bashar al-Assad tells the US it will 'pay the price' if it strikes against Syria.\"\n",
    "]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def custom_tokenizer_callback(text):\n",
    "    # Load the spaCy language model\n",
    " \n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # Learn the vocabulary and transform the documents into a bag-of-words matrix\n",
    "    bag_of_words_matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "    # Get the vocabulary (unique words) and their corresponding indices\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    return vocabulary\n",
    " \n",
    "print(\"\\nVocabulary:\")\n",
    "print(custom_tokenizer_callback(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "UjBJUUpcBWp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['about' 'aerial' 'against' 'al' 'are' 'aren' 'as' 'assad' 'bashar'\n",
      " 'birdview' 'canon' 'cellphones' 'cmos' 'contours' 'danger' 'days'\n",
      " 'digital' 'during' 'early' 'enabled' 'enthusiastic' 'features' 'find'\n",
      " 'from' 'gps' 'great' 'ground' 'heaps' 'identify' 'if' 'image' 'in' 'is'\n",
      " 'it' 'lake' 'land' 'leader' 'level' 'much' 'neighbourhood' 'of' 'or'\n",
      " 'order' 'paths' 'pay' 'photograph' 'photographs' 'photography' 'points'\n",
      " 'president' 'pretty' 'price' 'river' 'rubbish' 'sensor' 'slrs' 'specific'\n",
      " 'strikes' 'students' 'study' 'such' 'syria' 'syrian' 'take' 'taking'\n",
      " 'technology' 'teenagers' 'tells' 'terrestrial' 'that' 'the' 'their' 'to'\n",
      " 'undisputed' 'us' 'use' 'visible' 'was' 'way' 'will']\n",
      "Binary BoW Representation:\n",
      " [[0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1\n",
      "  0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
      "  1 0 0 1 0 0 0 0]\n",
      " [1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1\n",
      "  1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0\n",
      "  0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0\n",
      "  1 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      "  1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
      "  0 1 0 0 0 1 0 0]\n",
      " [0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0\n",
      "  0 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Initialize a CountVectorizer object and set it to use\n",
    "# your spacy_tokenizer with lower-casing off and to create a binary BOW.\n",
    "#\n",
    "\n",
    "# Instantiate a CountVectorizer object called 'vectorizer'.\n",
    "# Initialize CountVectorizer with binary=True\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Create a binary BOW from the corpus using your CountVectorizer.\n",
    "# Fit and transform the corpus\n",
    "binary_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to array for better readability\n",
    "binary_bow_array = binary_bow.toarray()\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display results\n",
    "print(\"Feature Names:\", feature_names)\n",
    "print(\"Binary BoW Representation:\\n\", binary_bow_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "os3tPj5nmRLw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aerial', 'are', 'as', 'become', 'bottles', 'breeding', 'cameras',\n",
       "       'can', 'carrying', 'children', 'common', 'danger', 'dengue',\n",
       "       'digital', 'enabled', 'favelas', 'fever', 'for', 'gps', 'ground',\n",
       "       'heaps', 'in', 'kites', 'launched', 'living', 'mosquitoes',\n",
       "       'neighbourhood', 'of', 'old', 'pictures', 'points', 'rubbish',\n",
       "       'shots', 'sitting', 'smartphones', 'specific', 'such', 'take',\n",
       "       'teenagers', 'the', 'their', 'then', 'they', 'to', 'toy', 'use',\n",
       "       'using', 'via', 'which'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# The string below is a whole paragraph. We want to create another\n",
    "# binary BOW but using the vocabulary of our *current* CountVectorizer. This means\n",
    "# that words in this paragraph which AREN'T already in the vocabulary won't be\n",
    "# represented. This is to illustrate how BOW can't handle out-of-vocabulary words\n",
    "# unless you rebuild your whole vocabulary. Still, we'll see that if there's\n",
    "# enough overlapping vocabulary, some similarity can still be picked up.\n",
    "#\n",
    "# Note that we call 'transform' only instead of 'fit_transform' because the\n",
    "# fit step (i.e. vocabulary build) is already done and we don't want to re-fit here.\n",
    "#\n",
    "corpus_s = [\n",
    "    \"Teenagers take aerial shots of their neighbourhood using digital cameras sitting in old bottles which are launched via kites - a common toy for children living in the favelas. \"\n",
    "    \"They then use GPS-enabled smartphones to take pictures of specific danger points - such as rubbish heaps, which can become a breeding ground for mosquitoes carrying dengue fever.\"\n",
    "    ]\n",
    "\n",
    "new_bow = vectorizer.transform(corpus_s)\n",
    "\n",
    "custom_tokenizer_callback(corpus_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Cosine Similarities:\n",
      "[[0.68181818]\n",
      " [0.41391868]\n",
      " [0.26673253]\n",
      " [0.20100756]\n",
      " [0.05330018]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: using the pairwise cosine_similarity method from sklearn,\n",
    "# calculate the similarities between each document from the corpus against\n",
    "# this new document (new_bow). HINT: You can pass two parameters to\n",
    "# cosine_similarity in this case. See the docs:\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine\n",
    "#\n",
    "# Which document is the most similar? Which is the least similar? Do the results make sense\n",
    "# based on what you see?\n",
    "#\n",
    "# Calculate pairwise cosine similarity\n",
    "similarities = cosine_similarity(binary_bow, new_bow)\n",
    "\n",
    "print(\"Pairwise Cosine Similarities:\")\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXThYmDiwMmR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: [[0.68181818 0.41391868 0.26673253 0.20100756 0.05330018]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Implement your own cosine similarity method using numpy.\n",
    "# It should take two numpy arrays and output the similarity metric.\n",
    "# HINTS:\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
    "#\n",
    "# Verify the similarity between the first document in the corpus and the\n",
    "# paragraph is the same as the one you got from using pairwise cosine_similarity.\n",
    "#\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "  # Calculate the cosine similarity\n",
    "  cosine_similarity = np.dot( a, b) / (norm(a) * norm(b))\n",
    "  return cosine_similarity\n",
    "\n",
    "print(\"Cosine Similarity:\", cosine_similarity(new_bow, binary_bow))\n",
    "# Pairwise Cosine Similarities:\n",
    "# [[0.68181818]\n",
    "#  [0.41391868]\n",
    "#  [0.26673253]\n",
    "#  [0.20100756]\n",
    "#  [0.05330018]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ghlqn6l-dal4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      "['aerial' 'al' 'assad' 'bashar' 'birdview' 'canon' 'cellphone' 'cmos'\n",
      " 'contour' 'danger' 'day' 'digital' 'early' 'enable' 'enthusiastic'\n",
      " 'feature' 'find' 'gps' 'great' 'ground' 'heap' 'identify' 'image' 'lake'\n",
      " 'land' 'leader' 'level' 'neighbourhood' 'order' 'path' 'pay' 'photograph'\n",
      " 'photography' 'point' 'president' 'pretty' 'price' 'river' 'rubbish'\n",
      " 'sensor' 'slrs' 'specific' 'strike' 'student' 'study' 'syria' 'syrian'\n",
      " 'take' 'technology' 'teenager' 'tell' 'terrestrial' 'undisputed' 'use'\n",
      " 'visible' 'way']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: In spacy_tokenizer, instead of returning the plain text,\n",
    "# return the lemma_ attribute instead. How do the cosine similarity\n",
    "# results differ? What if you filter out stop words as well?\n",
    "#\n",
    "\n",
    "# Custom preprocessing function using spaCy\n",
    "def spacy_preprocessor(text):\n",
    "    doc = nlp(text)\n",
    "    # Lemmatize and remove stopwords\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Preprocess text using spaCy\n",
    "preprocessed_texts = [spacy_preprocessor(text) for text in corpus]\n",
    "\n",
    "def custom_tokenizer_callback(text):\n",
    "    # Load the spaCy language model\n",
    " \n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # Learn the vocabulary and transform the documents into a bag-of-words matrix\n",
    "    bag_of_words_matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "    # Get the vocabulary (unique words) and their corresponding indices\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    return vocabulary\n",
    " \n",
    "print(\"\\nVocabulary:\")\n",
    "print(custom_tokenizer_callback(preprocessed_texts))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnC_i4oH2ARW"
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "Course module for this demo: https://www.nlpdemystified.org/course/tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb7W_O_FS3H6"
   },
   "source": [
    "**NOTE: If the notebook timed out, you may need to re-upgrade spaCy and re-install the language model as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRtp9F8KS5QE"
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy==3.*\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "CMwv39AfP7Ti"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmcTBtSx-XqZ"
   },
   "source": [
    "## Fetching datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYkq3i7_-qhQ"
   },
   "source": [
    "This time around, rather than using a short toy corpus, let's use a larger dataset. scikit-learn has a **datasets** module with utilties to load datasets of our own as well as fetch popular reference datasets online.<br>\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n",
    "<br><br>\n",
    "We'll use the **20 newsgroups** dataset, which is a collection of 18,000 newsgroup posts across 20 topics.<br>\n",
    "https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
    "<br><br>\n",
    "List of datasets available:<br>\n",
    "https://scikit-learn.org/stable/datasets.html#datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYjxqxVBBINV"
   },
   "source": [
    "The **datasets** module includes fetchers for each dataset in scikit-learn. For our purposes, we'll fetch only the posts from the *sci.space* topic, and skip on headers, footers, and quoting of other posts.<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\n",
    "<br><br>\n",
    "By default, the fetcher retrieves the *training* subset of the data only. If you don't know what that means, it'll become clear later in the course when we discuss modelling. For now, it doesn't matter for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "T9to6gQNCGiN"
   },
   "outputs": [],
   "source": [
    "corpus = fetch_20newsgroups(categories=['sci.space'],\n",
    "                            remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W989GHQxDvTW"
   },
   "source": [
    "We get back a **Bunch** container object containing the data as well as other information.<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html\n",
    "<br><br>\n",
    "The actual posts are accessed through the *data* attribute and is a list of strings, each one representing a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "POGdVmdIDuCK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "q6AgmbL0ES9I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of posts in our dataset.\n",
    "len(corpus.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "qAjM4uNDEXGf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nAny lunar satellite needs fuel to do regular orbit corrections, and when\\nits fuel runs out it will crash within months.  The orbits of the Apollo\\nmotherships changed noticeably during lunar missions lasting only a few\\ndays.  It is *possible* that there are stable orbits here and there --\\nthe Moon's gravitational field is poorly mapped -- but we know of none.\\n\\nPerturbations from Sun and Earth are relatively minor issues at low\\naltitudes.  The big problem is that the Moon's own gravitational field\\nis quite lumpy due to the irregular distribution of mass within the Moon.\",\n",
       " '\\nGlad to see Griffin is spending his time on engineering rather than on\\nritual purification of the language.  Pity he got stuck with the turkey\\nrather than one of the sensible options.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first two posts.\n",
    "corpus.data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH99M6cxCpsz"
   },
   "source": [
    "## Creating TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "vtnQX-wWDhGh"
   },
   "outputs": [],
   "source": [
    "# Like before, if we want to use spaCy's tokenizer, we need\n",
    "# to create a callback. Remember to upgrade spaCy if you need\n",
    "# to (refer to beginnning of file for commentary and instructions).\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# We don't need named-entity recognition nor dependency parsing for\n",
    "# this so these components are disabled. This will speed up the\n",
    "# pipeline. We do need part-of-speech tagging however.\n",
    "unwanted_pipes = [\"ner\", \"parser\"]\n",
    "\n",
    "# For this exercise, we'll remove punctuation and spaces (which\n",
    "# includes newlines), filter for tokens consisting of alphabetic\n",
    "# characters, and return the lemma (which require POS tagging).\n",
    "def spacy_tokenizer(doc):\n",
    "  with nlp.disable_pipes(*unwanted_pipes):\n",
    "    return [t.lemma_ for t in nlp(doc) if \\\n",
    "            not t.is_punct and \\\n",
    "            not t.is_space and \\\n",
    "            t.is_alpha]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il-0gY9LEiNv"
   },
   "source": [
    "Like the classes to create raw frequency and binary bag-of-words vectors, scikit-learn includes a similar class called **TfidfVectorizer** to create TF-IDF vectors from a corpus.<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "<br><br>\n",
    "The usage pattern is similar in that we call *fit_transform* on the corpus which generates the vocabulary dictionary (fit step), and generates the TF-IDF vectors (transform step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Shj6BS0BN6FU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\38067\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 30.1 s\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use the default settings of TfidfVectorizer.\n",
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "features = vectorizer.fit_transform(corpus.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "CZ9w4gh9sobB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9463\n"
     ]
    }
   ],
   "source": [
    "# The number of unique tokens.\n",
    "print(len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "6CxmKlPcNRLk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(593, 9463)\n"
     ]
    }
   ],
   "source": [
    "# The dimensions of our feature matrix. X rows (documents) by Y columns (tokens).\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "yJwnU8PZNdHU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 424)\t0.07006735123597327\n",
      "  (0, 4943)\t0.17755697785104502\n",
      "  (0, 7310)\t0.08827255510573831\n",
      "  (0, 5573)\t0.07462737620371114\n",
      "  (0, 3317)\t0.1987888389166129\n",
      "  (0, 8517)\t0.06551158102003457\n",
      "  (0, 2378)\t0.04343054547334542\n",
      "  (0, 6912)\t0.13559878838138195\n",
      "  (0, 5908)\t0.21554277358564625\n",
      "  (0, 1847)\t0.13559878838138195\n",
      "  (0, 370)\t0.1054358136369086\n",
      "  (0, 9237)\t0.0715855496878138\n",
      "  (0, 4402)\t0.07522156165875085\n",
      "  (0, 7244)\t0.0978911139378133\n",
      "  (0, 5963)\t0.0643662961391887\n",
      "  (0, 4393)\t0.07654434326236456\n",
      "  (0, 9274)\t0.059872496633831214\n",
      "  (0, 1902)\t0.13559878838138195\n",
      "  (0, 9311)\t0.1929427392927135\n",
      "  (0, 5402)\t0.10099174099290609\n",
      "  (0, 8393)\t0.20401777246040834\n",
      "  (0, 5817)\t0.09912761029075574\n",
      "  (0, 449)\t0.10452131953855516\n",
      "  (0, 5429)\t0.17101697764367227\n",
      "  (0, 1348)\t0.09035933266335426\n",
      "  :\t:\n",
      "  (0, 6381)\t0.1533078830125271\n",
      "  (0, 5048)\t0.12319463512940872\n",
      "  (0, 1145)\t0.04891599740875135\n",
      "  (0, 9181)\t0.06193123322498519\n",
      "  (0, 4611)\t0.06321352212439027\n",
      "  (0, 5679)\t0.11218863865345223\n",
      "  (0, 6207)\t0.13901035829688818\n",
      "  (0, 3303)\t0.051871597642995475\n",
      "  (0, 8133)\t0.09512638421732131\n",
      "  (0, 2499)\t0.07376123146042254\n",
      "  (0, 6934)\t0.1252396321004397\n",
      "  (0, 5299)\t0.13258957045064257\n",
      "  (0, 4389)\t0.10269562711579854\n",
      "  (0, 628)\t0.05104163249933749\n",
      "  (0, 4921)\t0.08207539077033385\n",
      "  (0, 316)\t0.11094210738480076\n",
      "  (0, 897)\t0.08929464431895895\n",
      "  (0, 6542)\t0.0845497898324203\n",
      "  (0, 6008)\t0.10182948237250997\n",
      "  (0, 6730)\t0.09939441945830645\n",
      "  (0, 4941)\t0.17101697764367227\n",
      "  (0, 2464)\t0.10861788552515572\n",
      "  (0, 4362)\t0.1533078830125271\n",
      "  (0, 2357)\t0.12746267037039388\n",
      "  (0, 5089)\t0.10452131953855516\n"
     ]
    }
   ],
   "source": [
    "# What the encoding of the first document looks like in sparse format.\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp7VTwYzONlt"
   },
   "source": [
    "As we mentioned in the slides, there are TF-IDF variations out there and scikit-learn, among other things, adds **smoothing** (adds a one to the numerator and denominator in the IDF component), and normalizes by default. These can be disabled if desired using the *smooth_idf* and *norm* parameters respectively. See here for more information:<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylKLM-IMOwbJ"
   },
   "source": [
    "## Querying the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8oTtCg0QB71"
   },
   "source": [
    "The similarity measuring techniques we learned previously can be used here in the same way. In effect, we can query our data using this sequence:\n",
    "1. *Transform* our query using the same vocabulary from our *fit* step on our corpus.\n",
    "2. Calculate the pairwise cosine similarities between each document in our corpus and our query.\n",
    "3. Sort them in descending order by score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "qNjEUzqlP6Oy"
   },
   "outputs": [],
   "source": [
    "# Transform the query into a TF-IDF vector.\n",
    "query = [\"lunar orbit\"]\n",
    "query_tfidf = vectorizer.transform(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "jEfdfkmpP8Tv"
   },
   "outputs": [],
   "source": [
    "# Calculate the cosine similarities between the query and each document.\n",
    "# We're calling flatten() here becaue cosine_similarity returns a list\n",
    "# of lists and we just want a single list.\n",
    "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skuSFhLxXOMC"
   },
   "source": [
    "Now that we have our list of cosine similarities, we can use this utility function to return the indices of the top k documents with the highest cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "H0PvqRDpUSYO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# numpy's argsort() method returns a list of *indices* that\n",
    "# would sort an array:\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
    "#\n",
    "# The sort is ascending, but we want the largest k cosine_similarites\n",
    "# at the bottom of the sort. So we negate k, and get the last k\n",
    "# entries of the indices list in reverse order. There are faster\n",
    "# ways to do this using things like argpartition but this is\n",
    "# more succinct.\n",
    "def top_k(arr, k):\n",
    "  kth_largest = (k + 1) * -1\n",
    "  return np.argsort(arr)[:kth_largest:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "zFYpEldVUaAG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[249 108   0 312 509]\n"
     ]
    }
   ],
   "source": [
    "# So for our query above, these are the top five documents.\n",
    "top_related_indices = top_k(cosine_similarities, 5)\n",
    "print(top_related_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "4e86P3bQR1ZS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47796463 0.42917994 0.27361651 0.19484941 0.19147591]\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at their respective cosine similarities.\n",
    "print(cosine_similarities[top_related_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "kzdyTptURiTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actually, Hiten wasn't originally intended to go into lunar orbit at all,\n",
      "so it indeed didn't have much fuel on hand.  The lunar-orbit mission was\n",
      "an afterthought, after Hagoromo (a tiny subsatellite deployed by Hiten\n",
      "during a lunar flyby) had a transmitter failure and its proper insertion\n",
      "into lunar orbit couldn't be positively confirmed.\n",
      "\n",
      "It should be noted that the technique does have disadvantages.  It takes\n",
      "a long time, and you end up with a relatively inconvenient lunar orbit.\n",
      "If you want something useful like a low circular polar orbit, you do have\n",
      "to plan to expend a certain amount of fuel, although it is reduced from\n",
      "what you'd need for the brute-force approach.\n"
     ]
    }
   ],
   "source": [
    "# Top match.\n",
    "print(corpus.data[top_related_indices[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "zQwWXypfR8vh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Their Hiten engineering-test mission spent a while in a highly eccentric\n",
      "Earth orbit doing lunar flybys, and then was inserted into lunar orbit\n",
      "using some very tricky gravity-assist-like maneuvering.  This meant that\n",
      "it would crash on the Moon eventually, since there is no such thing as\n",
      "a stable lunar orbit (as far as anyone knows), and I believe I recall\n",
      "hearing recently that it was about to happen.\n"
     ]
    }
   ],
   "source": [
    "# Second-best match.\n",
    "print(corpus.data[top_related_indices[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "w-5aqUbGSM5J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[378 138 248 539  61]\n",
      "[0.38932857 0.34067377 0.29841515 0.266025   0.25696839]\n"
     ]
    }
   ],
   "source": [
    "# Try a different query\n",
    "query = [\"satellite\"]\n",
    "query_tfidf = vectorizer.transform(query)\n",
    "\n",
    "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
    "top_related_indices = top_k(cosine_similarities, 5)\n",
    "\n",
    "print(top_related_indices)\n",
    "print(cosine_similarities[top_related_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "VHQtRQIcSbTj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "As an Amateur Radio operator (VHF 2metres) I like to keep up with what is \n",
      "going up (and for that matter what is coming down too).\n",
      " \n",
      "In about 30 days I have learned ALOT about satellites current, future and \n",
      "past all the way back to Vanguard series and up to Astro D observatory \n",
      "(space).  I borrowed a book from the library called Weater Satellites (I \n",
      "think, it has a photo of the earth with a TIROS type satellite on it.)\n",
      " \n",
      "I would like to build a model or have a large color poster of one of the \n",
      "TIROS satellites I think there are places in the USA that sell them.\n",
      "ITOS is my favorite looking satellite, followed by AmSat-OSCAR 13 \n",
      "(AO-13).\n",
      " \n",
      "TTYL\n",
      "73\n",
      "Jim\n"
     ]
    }
   ],
   "source": [
    "print(corpus.data[top_related_indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4v5wQ4JaBIh"
   },
   "source": [
    "So here we have the beginnings of a simple search engine but we're a far cry from competing with commercial off-the-shelf search engines, let alone Google.\n",
    "<br>\n",
    "- For each query, we're scanning through our entire corpus, but in practice, you'll want to create an **inverted index**. Search applications such as Elasticsearch do that under the hood.\n",
    "- You'd also want to evaluate the efficacy of your search using metrics like **precision** and **recall**.\n",
    "- Document ranking also tends to be more sophisticated, using different ranking functions like Okapi BM25. With major search engines, ranking also involves hundreds of variables such as what the user searched for previously, what do they tend to click on, where are they physically, and on and on. These variables are part of the \"secret sauce\" and are closely guarded by companies.\n",
    "- Beyond word presence, intent and meaning are playing a larger role.\n",
    "<br>\n",
    "\n",
    "Information Retrieval is a huge, rich topic and beyond search, it's also key in tasks such as question-answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak3LXiETfGIY"
   },
   "source": [
    "## TF-IDF Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08nTQB7_fJU0"
   },
   "source": [
    "**EXERCISE**<br>\n",
    "Read up on these concepts we just mentioned if you're curious.<br>\n",
    "\n",
    "https://en.wikipedia.org/wiki/Inverted_index<br>\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall<br>\n",
    "https://en.wikipedia.org/wiki/Okapi_BM25<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Iz2FCCq1fsjz"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: fetch multiple topics from the 20 newsgroups\n",
    "# dataset and query them using the approach we followed.\n",
    "# A list of topics can be found here:\n",
    "# https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
    "#\n",
    "# If you're feeling ambitious, incorporate n-grams or\n",
    "# look at how you can measure precision and recall.\n",
    "#\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Download the dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fetch_20newsgroups function returns a Bunch object, which is similar to a dictionary. \n",
    "# It contains a few key and useful attributes:\n",
    "\n",
    "        # data: This contains the raw newsgroup text files.\n",
    "        # target: This holds the category indices of the lists.\n",
    "        # target_names: It lists the category names corresponding to each index in target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category names:  ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "First text sample:  From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Corresponding category:  7\n"
     ]
    }
   ],
   "source": [
    "# Exploring the datasets\n",
    "print(\"Category names: \", newsgroups_train.target_names)\n",
    "print(\"First text sample: \", newsgroups_train.data[0])\n",
    "print(\"Corresponding category: \", newsgroups_train.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the Text Data\n",
    "# One requires converting the text into a structured format that can be understood by a model, \n",
    "# typically using techniques like text vectorization (e.g., Bag of Words, TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train the model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, newsgroups_train.target)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8169144981412639\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.69      0.74       319\n",
      "           comp.graphics       0.78      0.72      0.75       389\n",
      " comp.os.ms-windows.misc       0.79      0.72      0.75       394\n",
      "comp.sys.ibm.pc.hardware       0.68      0.81      0.74       392\n",
      "   comp.sys.mac.hardware       0.86      0.81      0.84       385\n",
      "          comp.windows.x       0.87      0.78      0.82       395\n",
      "            misc.forsale       0.87      0.80      0.83       390\n",
      "               rec.autos       0.88      0.91      0.90       396\n",
      "         rec.motorcycles       0.93      0.96      0.95       398\n",
      "      rec.sport.baseball       0.91      0.92      0.92       397\n",
      "        rec.sport.hockey       0.88      0.98      0.93       399\n",
      "               sci.crypt       0.75      0.96      0.84       396\n",
      "         sci.electronics       0.84      0.65      0.74       393\n",
      "                 sci.med       0.92      0.79      0.85       396\n",
      "               sci.space       0.82      0.94      0.88       394\n",
      "  soc.religion.christian       0.62      0.96      0.76       398\n",
      "      talk.politics.guns       0.66      0.95      0.78       364\n",
      "   talk.politics.mideast       0.95      0.94      0.94       376\n",
      "      talk.politics.misc       0.94      0.52      0.67       310\n",
      "      talk.religion.misc       0.95      0.24      0.38       251\n",
      "\n",
      "                accuracy                           0.82      7532\n",
      "               macro avg       0.84      0.80      0.80      7532\n",
      "            weighted avg       0.83      0.82      0.81      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(newsgroups_test.target, predictions)\n",
    "report = classification_report(newsgroups_test.target, predictions, target_names=newsgroups_test.target_names)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_vectorization.ipynb",
     "timestamp": 1748500463424
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
