{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM8kLxUEVc3Z"
   },
   "source": [
    "# Natural Language Processing Demystified | Preprocessing\n",
    "https://nlpdemystified.org<br>\n",
    "https://github.com/futuremojo/nlp-demystified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btimL_w92Q3P"
   },
   "source": [
    "### spaCy upgrade and package installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7Ll-fUK2VZs"
   },
   "source": [
    "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy.\n",
    "<br><br>\n",
    "**IMPORTANT**<br>\n",
    "If you're running this in the cloud rather than using a local Jupyter server on your machine, then the notebook will **timeout** after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical packages.\n",
    "<br><br>\n",
    "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cve1-G7j2VTN"
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy==3.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "z-FDdbc62VHd"
   },
   "outputs": [],
   "source": [
    "# !python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8vW9svTE289D"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfJKSJEU2U_s"
   },
   "source": [
    "After importing spaCy, the next thing we need to do is load a suitable statistical model for our project. spaCy offers a variety of models for different languages. These models help with tokenization, part-of-speech tagging, named entity recognition, and more.\n",
    "\n",
    "Here, we're loading the **en_core_web_sm** model which is the smallest English model spaCy offers and is a good starting point for NLP tasks.<br>\n",
    "https://spacy.io/models/en#en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v6TGQff2iu6"
   },
   "source": [
    "Since we upgraded spaCy, we'll need to download the statistical model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4uOyHDNb2i5d"
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mWDrpxDk2_r2"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7YCbWtG3LJO"
   },
   "source": [
    "**en_core_web_sm** is trained on OntoNotes 5 which is an annotated corpus comprising news, blogs, transcripts, etc. Put simply, this means a bunch of documents were labelled with information such as how each sentence should be parsed, whether a particular word is a noun or adjective or other part-of-speech, whether a word is a special entity like a person or a real-world organization, and other language-related labels. A statistical model was then generated from these labelled documents.<br>\n",
    "https://catalog.ldc.upenn.edu/LDC2013T19\n",
    "<br><br>\n",
    "You can learn more about the available spaCy models at these links:<br>\n",
    "https://spacy.io/models<br>\n",
    "https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvF_udvi3OTO"
   },
   "source": [
    "After loading the model, the _nlp_ variable now references a **Language** class instance which contains language-specific rules for various tasks (e.g. tokenization) and a processing pipeline.<br>\n",
    "https://spacy.io/api/language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DAYGtQpT3UNN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unmnGRu8D-wa"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLUcGm3IbQki"
   },
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13twUCp2i_p8"
   },
   "source": [
    "We pass whatever text we want to process to _nlp_, which returns a **Doc** container object containing the tokenized text and a number of annotations for each token. These annotations are discussed in follow-up videos. You can learn more about the **Doc** object here:<br>\n",
    "https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BIoEJZ-IkHQ4"
   },
   "outputs": [],
   "source": [
    "# Sample sentence.\n",
    "s = \"He didn't want to pay $20 for this book.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMWZK3ZSk9-f"
   },
   "source": [
    "We can iterate over this **Doc** object and view the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8SzqhZuulAe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'did', \"n't\", 'want', 'to', 'pay', '$', '20', 'for', 'this', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai1obkB93GdD"
   },
   "source": [
    "Note how\n",
    "- \"didn't\" is separated into \"did\"  and \"n't\".\n",
    "- the currency symbol and amount are separated.\n",
    "- the period at the end of the sentence is its own token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWH49gIh3hqN"
   },
   "source": [
    "The **Doc** object can be indexed and sliced like a regular list. The **Doc** object contains **Token** and **Span** objects, which offer different views into the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MwLrxRsE3oKI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "want\n"
     ]
    }
   ],
   "source": [
    "# We can view an individual token by indexing into the Doc object.\n",
    "print(doc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bGapNHYQFYVa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# A Doc object is a container of other objects, namely Token and Span objects.\n",
    "print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "EtL2IgIAGOd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't want\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# Slicing a Doc object returns a Span object.\n",
    "print(doc[0:4])\n",
    "print(type(doc[0:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "xybH4jjYGo73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 0), ('did', 1), (\"n't\", 2), ('want', 3), ('to', 4), ('pay', 5), ('$', 6), ('20', 7), ('for', 8), ('this', 9), ('book', 10), ('.', 11)]\n"
     ]
    }
   ],
   "source": [
    "# Access a token's index in a sentence.\n",
    "print([(t.text, t.i) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TqE980F4Vrt"
   },
   "source": [
    "Spacy's tokenization is _non-destructive_, which means the original input can be reconstructed from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "OjXb8mR_DK-1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't want to pay $20 for this book.\n"
     ]
    }
   ],
   "source": [
    "# You can view the original input like so:\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73vuSX7MDK79"
   },
   "source": [
    "You can learn more about the **Token** and **Span** objects here:<br>\n",
    "https://spacy.io/api/token<br>\n",
    "https://spacy.io/api/span\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lume_1UP6ySQ"
   },
   "source": [
    "We can also tokenize multiple sentences and access each sentence individually using the **Doc** object's _sents_ property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "mPZ86x0hDK4m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Either the well was very deep, or she fell very slowly, for she\n",
      "had plenty of time as she went down to look about her and to wonder what\n",
      "was going to happen next., First, she tried to look down and make out what\n",
      "she was coming to, but it was too dark to see anything; then she looked at\n",
      "the sides of the well, and noticed that they were filled with cupboards and\n",
      "book-shelves; here and there she saw maps and pictures hung upon pegs.]\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"Either the well was very deep, or she fell very slowly, for she\n",
    "had plenty of time as she went down to look about her and to wonder what\n",
    "was going to happen next. First, she tried to look down and make out what\n",
    "she was coming to, but it was too dark to see anything; then she looked at\n",
    "the sides of the well, and noticed that they were filled with cupboards and\n",
    "book-shelves; here and there she saw maps and pictures hung upon pegs.\"\"\"\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "# Look at individual sentences (there should be two 'Span' objects).\n",
    "print([sent for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSfDUyK06Qg"
   },
   "source": [
    "### Tokenization Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyywcBrCHzSk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$20\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE:\n",
    "# 1) Tokenize the following text\n",
    "# 2) Iterate through the tokens to check whether there's a currency symbol.\n",
    "# 3) If there is, and the currency label is followed by a number, print\n",
    "#    both the symbol and the number.\n",
    "#\n",
    "# Look through https://spacy.io/api/token#attributes on how to check whether\n",
    "# a token is a currency symbol or a number.\n",
    "#\n",
    "s = \"He didn't want to pay $20 for this book.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)  \n",
    "\n",
    "pattern = [{'TEXT':'$'}, {'IS_DIGIT': True}, ] # $ + NUMBER\n",
    "\n",
    "matcher.add(\"CurrencyDollar\", [pattern])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "skajI-OZDK0t"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Learn how the spaCy tokenizer works and how to customize it:\n",
    "# https://spacy.io/usage/linguistic-features#tokenization\n",
    "#\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "prefix_re = re.compile(r'''^[\\\\[\\\\(\"']''')\n",
    "suffix_re = re.compile(r'''[\\\\]\\\\\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'world.', ':)']\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                                prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                url_match=simple_url_re.match)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(\"hello-world. :)\")\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ikbnyb8rDKv9"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read through spaCy-101 and if you're interested, check out their course\n",
    "# on spaCy itself (link on the page).\n",
    "# https://spacy.io/usage/spacy-101\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "MMArLP91DKUW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'s\", 'go', 'to', 'N.Y.C.', 'for', 'the', 'weekend', '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Look up how to tokenize the sentence below using NLTK. The imports\n",
    "# are done for you. Does the NLTK tokenizer handle \"N.Y.C.\" correctly?\n",
    "#\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "# The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n",
    "\n",
    "# This tokenizer performs the following steps:\n",
    "\n",
    "# split standard contractions, e.g. don't -> do n't and they'll -> they 'll\n",
    "\n",
    "# treat most punctuation characters as separate tokens\n",
    "\n",
    "# split off commas and single quotes, when followed by whitespace\n",
    "\n",
    "# separate periods that appear at the end of line\n",
    "s = \"Let's go to N.Y.C. for the weekend.\"\n",
    "TreebankWordTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMbm9tTakDdy"
   },
   "source": [
    "**NOTE**: Different tokenizers will give subtly different results based on the rules they use. Experiment with different tokenizers and use the one best suited for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUsfYCpVT4nI"
   },
   "source": [
    "# Basic Preprocessing\n",
    "## Case-Folding, Stop Word Removal, Stemming, and Lemmatization.\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/basic-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gaj23tgd7Su"
   },
   "source": [
    "**NOTE: If the notebook timed out, you may need to re-upgrade spaCy and re-install the language model as follows:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mg6dga4JePf2"
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy==3.*\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgDDrCeI8f-4"
   },
   "source": [
    "spaCy performs all these preprocessing steps (except stemming) behind the scenes for you. Inline with its non-destructive policy, the tokens aren't modified directly. Rather, each **Token** object has a number of attributes which can help you get views of your document with these pre-processing steps applied. The attributes a **Token** has can be found here:<br>\n",
    "https://spacy.io/api/token#attributes\n",
    "<br><br>\n",
    "More information about spaCy's processing pipeline:<br>\n",
    "https://spacy.io/usage/processing-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "jDEMR6En1j3H"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"He told Dr. Lovato that he was done with the tests and would post the results shortly.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwA1ct0obYlR"
   },
   "source": [
    "### Case-Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biBPWrVd9BrK"
   },
   "source": [
    "View your document with case-folding using the *lower_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "1nt4RpzdgQQL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
     ]
    }
   ],
   "source": [
    "print([t.lower_ for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL46I4sH9OMq"
   },
   "source": [
    "You can also apply conditions when generating these views. For example, we can skip case-folding if a token is the start of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "IO0PQ8IFhOlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[He, 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
     ]
    }
   ],
   "source": [
    "print([t.lower_ if not t.is_sent_start else t for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7pTz8XJbmaT"
   },
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZLqqmHa9cRx"
   },
   "source": [
    "spaCy comes with a default stop word list. To view your document with stop words removed, you can use the *is_stop* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "9kvXbuDEhOxu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'’s', 'everyone', 'our', 'wherever', 'though', 'myself', 'anywhere', 'often', 'why', 'their', 'herein', 'own', 'alone', 'also', 'two', 'you', 'cannot', 'beforehand', 'yourself', 'well', 'upon', 'three', 'rather', 'almost', \"n't\", 'hereby', 'make', 'name', 'regarding', 'another', 'every', 'can', 'thereafter', 'perhaps', 'be', 'per', 'through', 'anyway', '’ve', 'among', 'such', 'of', 'none', 'done', 'enough', 'for', 'either', 'was', 'ca', 'were', 'anything', 'several', 'former', 'latter', 'onto', 'go', 'back', 'what', 'under', 'anyone', 'did', 'hereupon', 'or', 'whoever', 'thru', 'further', 'after', 'nevertheless', 'with', 'has', 'am', 'my', 'became', 'above', \"'d\", 'say', 'third', 'whenever', 'somehow', 'by', 'as', '‘re', 'using', 'somewhere', 'where', 'her', 'from', 'whereby', 'many', 'amongst', 'once', 'again', 'without', 'it', 'must', 'however', 'show', 'hereafter', 'really', 'if', 'due', 'part', 'therein', 'sixty', '’m', 'eight', 'wherein', 'himself', 'except', 'too', 'nor', 'full', 'that', 're', '‘s', 'thence', 'she', 'over', 'twelve', 'i', 'thus', 'between', 'whence', 'this', 'hundred', 'used', 'become', 'the', 'since', 'toward', 'last', 'most', 'serious', 'but', 'within', 'seem', 'about', 'herself', 'so', 'on', 'at', '’d', 'each', 'down', 'sometime', 'never', 'they', 'me', 'therefore', 'there', 'thereupon', \"'ll\", 'get', 'someone', '‘ll', 'already', 'move', 'is', 'off', 'else', 'whom', 'mostly', 'them', 'top', 'both', 'against', 'everywhere', '‘m', 'take', 'up', 'empty', 'may', 'please', 'unless', 'he', 'otherwise', 'then', 'around', 'keep', 'always', 'because', 'call', 'no', 'something', 'although', 'itself', 'same', 'becoming', 'elsewhere', 'n‘t', 'along', 'even', 'now', 'all', 'during', 'those', 'ourselves', 'twenty', 'ours', 'out', 'noone', 'very', 'n’t', 'namely', 'than', 'sometimes', 'whether', 'will', 'least', 'afterwards', 'until', 'mine', 'whereafter', 'which', 'latterly', 'only', 'whose', 'amount', 'had', 'much', 'fifty', 'next', 'whatever', 'these', 'an', 'four', 'throughout', 'nowhere', 'whereupon', \"'ve\", 'beside', 'seeming', 'whole', \"'s\", 'via', 'yet', 'first', 'beyond', 'quite', 'would', 'neither', 'hence', '‘d', 'a', 'others', 'any', 'seemed', 'thereby', 'does', 'across', \"'m\", 'indeed', 'everything', 'being', 'have', 'seems', 'here', 'bottom', 'nobody', 'nine', 'yourselves', 'before', 'ever', 'less', 'whereas', 'his', 'formerly', 'when', 'see', 'eleven', 'various', 'who', 'give', 'behind', 'whither', 'doing', 'front', 'anyhow', 'nothing', 'six', '’ll', 'fifteen', 'more', 'might', 'him', '‘ve', 'us', 'how', 'not', 'hers', 'five', 'other', 'just', 'been', 'we', 'should', 'below', 'towards', 'becomes', 'some', \"'re\", 'one', 'yours', 'few', 'themselves', 'could', '’re', 'made', 'do', 'besides', 'into', 'ten', 'its', 'side', 'still', 'together', 'forty', 'put', 'and', 'are', 'your', 'to', 'in', 'while', 'meanwhile', 'moreover'}\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "# spaCy's default stop word list.\n",
    "print(nlp.Defaults.stop_words)\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "oAS1xmgOhO5y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[He, that, he, was, done, with, the, and, would, the]\n"
     ]
    }
   ],
   "source": [
    "print([t for t in doc if t.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[told, Dr., Lovato, tests, post, results, shortly, .]\n"
     ]
    }
   ],
   "source": [
    "print([t for t in doc if not t.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPd1aiLrbqcK"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKidP32Y_qcE"
   },
   "source": [
    "It's similar with lemmatization. You can view your document with lemmatization applied through the *lemma_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "fhdRleESkzTu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'he'),\n",
       " ('told', 'tell'),\n",
       " ('Dr.', 'Dr.'),\n",
       " ('Lovato', 'Lovato'),\n",
       " ('that', 'that'),\n",
       " ('he', 'he'),\n",
       " ('was', 'be'),\n",
       " ('done', 'do'),\n",
       " ('with', 'with'),\n",
       " ('the', 'the'),\n",
       " ('tests', 'test'),\n",
       " ('and', 'and'),\n",
       " ('would', 'would'),\n",
       " ('post', 'post'),\n",
       " ('the', 'the'),\n",
       " ('results', 'result'),\n",
       " ('shortly', 'shortly'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.lemma_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuaQJPjEjADE"
   },
   "source": [
    "### Basic Preprocessing Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uNdkuJqCA2A"
   },
   "source": [
    "spaCy doesn't support stemming natively. But for completeness, we can stem using **NLTK**. Specifically, we can use the *Snowball stemmer* which is an improved version of the *Porter stemmer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "_HQzMurVB13l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He --> he\n",
      "told --> told\n",
      "Dr. --> dr.\n",
      "Lovato --> lovato\n",
      "that --> that\n",
      "he --> he\n",
      "was --> was\n",
      "done --> done\n",
      "with --> with\n",
      "the --> the\n",
      "tests --> test\n",
      "and --> and\n",
      "would --> would\n",
      "post --> post\n",
      "the --> the\n",
      "results --> result\n",
      "shortly. --> shortly.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Find out how to intialize the SnowballStemmer, then tokenize\n",
    "# and stem the sentence below.\n",
    "#\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s = 'He told Dr. Lovato that he was done with the tests and would post the results shortly.'\n",
    " \n",
    "# Initialize the stemmer here.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Tokenize, stem, and print the tokens.\n",
    "words = s.split(\" \")\n",
    "#stem's of each word\n",
    "for word in words:\n",
    "    print(word, \"-->\", stemmer.stem(word) )\n",
    " \n",
    " \n",
    "#print stemming results\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "wOXJI061npqN"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Find out how to add and remove your own stop words in spaCy. Add the\n",
    "# word 'told' as a stop word, test that it works, then remove it from\n",
    "# the stop word list.\n",
    "#\n",
    "# Add custom stop words\n",
    "custom_stop_words = {\"told\"}\n",
    "for word in custom_stop_words:\n",
    "    nlp.Defaults.stop_words.add(word)\n",
    "    nlp.vocab[word].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'told', 'that', 'he', 'was', 'done', 'with', 'the', 'and', 'would', 'the']\n"
     ]
    }
   ],
   "source": [
    "s = 'He told Dr. Lovato that he was done with the tests and would post the results shortly.'\n",
    "# Test\n",
    "doc = nlp(s)\n",
    "print([token.text for token in doc if token.is_stop])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove specific stop words\n",
    "stop_words_to_remove = {\"told\"}\n",
    "\n",
    "for word in stop_words_to_remove:\n",
    "    nlp.Defaults.stop_words.remove(word)\n",
    "    nlp.vocab[word].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'that', 'he', 'was', 'done', 'with', 'the', 'and', 'would', 'the']\n"
     ]
    }
   ],
   "source": [
    "s = 'He told Dr. Lovato that he was done with the tests and would post the results shortly.'\n",
    "# Test\n",
    "doc = nlp(s)\n",
    "print([token.text for token in doc if token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLcCYIy-lP1u"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read up on how to add your own custom attributes to Token objects\n",
    "# and try adding one of your own.\n",
    "# https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9HLYYUt1kOP"
   },
   "source": [
    "#Advanced Preprocessing\n",
    "\n",
    "## Part-of-Speech Tagging, Named Entity Recognition, and Parsing.\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/advanced-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfBqaH9feymn"
   },
   "source": [
    "**NOTE: If the notebook timed out, you may need to re-upgrade spaCy and re-install the language model as follows:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xOdySsre_sP"
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy==3.*\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr5SqjHwSWpI"
   },
   "source": [
    "spaCy performs Part-of-Speech (POS) tagging, Named Entity Recognition (NER), and parsing as part of its default pipeline in the *nlp* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "shgWRMCq1kmy"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"John watched an old movie at the cinema.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwMQgciGb3or"
   },
   "source": [
    "### Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA9LDzULTW1_"
   },
   "source": [
    "POS tags can be accessed through the *pos_* attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "0-9YRcSZ1kqq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'PROPN'),\n",
       " ('watched', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('old', 'ADJ'),\n",
       " ('movie', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('cinema', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.pos_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UZcgwejnYm8"
   },
   "source": [
    "To get a description for a POS tag, we can use _spacy.explain_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "D9SXNvnmnW5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_WbFDZ-Tqu9"
   },
   "source": [
    "The POS tags above are called *course-grained* tags. You can also access *fine-grained* tags through the *tag_* attribute. Fine-grained tags provide more detailed information about a token such as its tense and, if a word is a pronoun, what specific type of pronoun it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "1Z5oDzNr1kt2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('watched', 'VBD'),\n",
       " ('an', 'DT'),\n",
       " ('old', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('cinema', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.tag_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPOaN9yOUN-I"
   },
   "source": [
    "So **NNP** refers specifically to a _singular pronoun_, and **VBD** is a verb in *past tense*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "pnfLDxoG1kxf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun, proper singular\n",
      "verb, past tense\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('NNP'))\n",
    "print(spacy.explain('VBD'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jte6K6HJb750"
   },
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J2BjPyqWFEf"
   },
   "source": [
    "There are multiple ways to access named entities. One way is through the *ent_type_* attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "dWjNrX6koNVj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Volkswagen', 'ORG'),\n",
       " ('is', ''),\n",
       " ('developing', ''),\n",
       " ('an', ''),\n",
       " ('electric', ''),\n",
       " ('sedan', ''),\n",
       " ('which', ''),\n",
       " ('could', ''),\n",
       " ('potentially', ''),\n",
       " ('come', ''),\n",
       " ('to', ''),\n",
       " ('America', 'GPE'),\n",
       " ('next', 'DATE'),\n",
       " ('fall', 'DATE'),\n",
       " ('.', '')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Volkswagen is developing an electric sedan which could potentially come to America next fall.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "[(t.text, t.ent_type_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJL4wfS6Wp9N"
   },
   "source": [
    "You can view spaCy's named entities annotations here:<br>\n",
    "https://spacy.io/api/annotation#named-entities\n",
    "\n",
    "or use _spacy.explain_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "iu4OiPwDo9So"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7p8IcNGpBTP"
   },
   "source": [
    "You can also check if a token is an entity before printing it by checking whether the _ent_type_ (note the lack of trailing underscore) attribute is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "0aBng8zdvjly"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Volkswagen', 'ORG'), ('America', 'GPE'), ('next', 'DATE'), ('fall', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "print([(t.text, t.ent_type_) for t in doc if t.ent_type != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lNS65_2XJIY"
   },
   "source": [
    "Another way is through the _ents_ property of the **Doc** object. Here, we iterate through _ents_ and print the entity itself and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "kSCzxs02vjdL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Volkswagen', 'ORG'), ('America', 'GPE'), ('next fall', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHfZmta8XX9Y"
   },
   "source": [
    "Note how \"next fall\" is outputted above as a single span when you use _ents_.\n",
    "<br><br>\n",
    "You can also access the positions of entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "mSzwRD0MvjTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Volkswagen', 'ORG', 0, 10), ('America', 'GPE', 75, 82), ('next fall', 'DATE', 83, 92)]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvvQ9_7FdEHT"
   },
   "source": [
    "spaCy is bundled with visualizers for both parsing and named entities.<br>\n",
    "https://spacy.io/usage/visualizers\n",
    "<br><br>\n",
    "Here, we visualize the entities in our sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "87eLywmVZCdw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Volkswagen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is developing an electric sedan which could potentially come to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    next fall\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# We need to set the 'jupyter' variable to True in order to output\n",
    "# the visualization directly. Otherwise, you'll get raw HTML.\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkNmqelTwTLZ"
   },
   "source": [
    "For domain-specific corpora, an NER tagger may need to be further fine-tuned. Here, we may want _The Martian_ tagged as a \"FILM\" (assuming that's our goal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "0bcIaah29MME"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ridley Scott\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " directed The \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Martian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = \"Ridley Scott directed The Martian.\"\n",
    "doc = nlp(s)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noGuG3JvcEfs"
   },
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppWrztdJeO3J"
   },
   "source": [
    "Let's first visualize a parse to make it easier to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "xrvfA1TEvjJT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cf4d619d35954589ad4a292430404845-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">She</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">enrolled</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">course</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">university.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cf4d619d35954589ad4a292430404845-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cf4d619d35954589ad4a292430404845-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cf4d619d35954589ad4a292430404845-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cf4d619d35954589ad4a292430404845-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cf4d619d35954589ad4a292430404845-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cf4d619d35954589ad4a292430404845-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cf4d619d35954589ad4a292430404845-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cf4d619d35954589ad4a292430404845-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cf4d619d35954589ad4a292430404845-0-4\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cf4d619d35954589ad4a292430404845-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cf4d619d35954589ad4a292430404845-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cf4d619d35954589ad4a292430404845-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cf4d619d35954589ad4a292430404845-0-6\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cf4d619d35954589ad4a292430404845-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = \"She enrolled in the course at the university.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Note the 'style' argument is assigned a 'dep' flag this time around.\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRN7_SQ-fO5H"
   },
   "source": [
    "The visualization above is for a dependency parse (spaCy doesn't come with a constituency parser). For each pair of depencencies, spaCy visualizes the child (pointed to), the head (pointed from), and their relationship (the label arc). You can view the dependency annotations here:<br>\n",
    "https://spacy.io/api/annotation#dependency-parsing\n",
    "\n",
    "You can also use *spacy.explain* to get information on a particular annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "wvz1bLTZfqmv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCvHyqHggIpd"
   },
   "source": [
    "The dependency labels themselves can be accessed through the *dep_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "iX_BgpMVoNaj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'nsubj'),\n",
       " ('enrolled', 'ROOT'),\n",
       " ('in', 'prep'),\n",
       " ('the', 'det'),\n",
       " ('course', 'pobj'),\n",
       " ('at', 'prep'),\n",
       " ('the', 'det'),\n",
       " ('university', 'pobj'),\n",
       " ('.', 'punct')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.dep_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt7zLq0ugR7O"
   },
   "source": [
    "Note how the word 'enrolled' is the _ROOT_.\n",
    "<br><br>\n",
    "But the labels above don't show how the words are related to each other (the arcs). To get a better idea, you can print the head of each dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "X15EOIq0oNfF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'nsubj', 'enrolled'),\n",
       " ('enrolled', 'ROOT', 'enrolled'),\n",
       " ('in', 'prep', 'enrolled'),\n",
       " ('the', 'det', 'course'),\n",
       " ('course', 'pobj', 'in'),\n",
       " ('at', 'prep', 'enrolled'),\n",
       " ('the', 'det', 'university'),\n",
       " ('university', 'pobj', 'at'),\n",
       " ('.', 'punct', 'enrolled')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.dep_, t.head.text) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFXPL37Rg2xm"
   },
   "source": [
    "### Using spaCy's Matcher to find patterns\n",
    "spaCy comes with a host of pattern-matching functionality. Beyond regex, spaCy can match on a variety of attributes such as POS tags, entity labels, lemmas, dependencies, entire phrases, and a lot more. You can learn more here:<br>\n",
    "https://spacy.io/usage/rule-based-matching<br>\n",
    "https://explosion.ai/demos/matcher\n",
    "<br><br>\n",
    "Here, we try to search for patterns that may be useful for a hospitality bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "6v4hVnYmJuaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['book a hotel', 'book a hotel room']\n"
     ]
    }
   ],
   "source": [
    "# The general Matcher is one of multiple matcher objects\n",
    "# included with spaCy.\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# We initialize the Matcher with the spaCy vocab object, which contains\n",
    "# words along with their labels and entities.\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "s = \"I want to book a hotel room.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Patterns are expressed as an ordered sequence. Here, we're looking\n",
    "# to match occurrences starting with a 'book' string followed by\n",
    "# a determiner (DET) POS tag, then a noun POS tag.\n",
    "# The OP key marks the match as optional in some way.\n",
    "\n",
    "# Here, the DET POS (marked with '?') will match 0 or 1 times, and\n",
    "# the NOUN POS (marked with '+') will match 1 or more times.\n",
    "# See this link for more information:\n",
    "# https://spacy.io/usage/rule-based-matching#quantifiers\n",
    "pattern = [\n",
    "  {'TEXT': 'book'},\n",
    "  {'POS': 'DET', 'OP': '?'},\n",
    "  {'POS': 'NOUN', 'OP': '+'},\n",
    "]\n",
    "\n",
    "# We give our pattern a label and pass it to the matcher.\n",
    "matcher.add('USER_INTENT', [pattern])\n",
    "\n",
    "# Run the matcher over the doc.\n",
    "matches = matcher(doc)\n",
    "\n",
    "# For each match, the matcher returns a tuple specifying a match id, start,\n",
    "# and end of the match.\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dygcIKF9plib"
   },
   "source": [
    "The code above demonstrates the Matcher but is brittle.\n",
    "- What if \"book\" is capitalized?\n",
    "- What if a user types \"reserve\" instead of \"book\"?\n",
    "- How can we match on \"hotel room\" as a compound noun?\n",
    "- What if a user types \"book a flight and hotel room\"?\n",
    "\n",
    "Can you think of how you would handle these cases?\n",
    "<br><br>\n",
    "We could come up more rules to match different patterns, or perhaps just search for keywords based on POS and entities (e.g. a country) and present the user with a bunch of possible intentions and let them choose one, or have a bunch of different interpretation functions submit answers and select the most likely one based on what was historically accepted most often. We can also ask clarifying questions to narrow things down.\n",
    "<br><br>\n",
    "For example, for the last sentence, you could have a function scan through the **Doc** object's *noun_chunks* (phrases that have a noun as their head) and isolate keywords there along with potential conjunctions (e.g. \"and\").<br>\n",
    "https://spacy.io/usage/linguistic-features#noun-chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "xctXGD5K5Gvr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase: I, root head: want\n",
      "phrase: a flight and hotel room, root head: book\n",
      "phrase: Berlin, root head: in\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I want to book a flight and hotel room in Berlin.\")\n",
    "for noun_phrase in doc.noun_chunks:\n",
    "  print(\"phrase: {}, root head: {}\".format(noun_phrase, noun_phrase.root.head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMsHWX-9EvXU"
   },
   "source": [
    "Using pure rules is a good place to start or prototype (especially if the domain is narrow with a tight set of use cases) but as our requirements get more sophisticated, we'll need to blend in other approaches such as classical models or perhaps deep learning (at the very least, maybe tune existing neural networks). spaCy's models can be updated with more examples to fine-tune predictions.<br>\n",
    "https://spacy.io/usage/training<br>\n",
    "<br>\n",
    "We'll keep learning more approaches as the course progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knyuUv9cqsoY"
   },
   "source": [
    "### Talkin' like Yoda\n",
    "Languages like English are built around the _subject-verb-object_ pattern. But if you're familiar with Yoda from Star Wars, he famously speaks in an _object-subject-verb pattern_. Using the information in a dependency parse, we can turn basic English sentences into Yoda-speak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "L9AydbEIqsRQ"
   },
   "outputs": [],
   "source": [
    "def yodize(s: str):\n",
    "  doc = nlp(s)\n",
    "  for t in doc:\n",
    "    if t.dep_ == \"ROOT\":\n",
    "\n",
    "      # Assuming our sentence is of the form subject-verb-object, we take\n",
    "      # everything after the root (likely verb) and put it in front, and\n",
    "      # likewise take everything before the root, and put it after.\n",
    "      seq = [doc[t.i + 1: -1].text, doc[0: t.i].text, t.text + '.']\n",
    "      seq[0] = seq[0].capitalize()\n",
    "      print(' '.join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "uIa8Cziwqqnf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To texas I will fly.\n"
     ]
    }
   ],
   "source": [
    "yodize(\"I will fly to Texas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofEnieaJZ8eX"
   },
   "source": [
    "This is ok for simple sentences but starts getting weird with longer, more convoluted sentences. What are some ways you would improve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V92TUxWioNtq"
   },
   "source": [
    "### Advanced Preprocessing Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ltil7XSyzMe"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Learn how to extend spaCy's NER models. Specifically, how to add new\n",
    "# entity names and entity types.\n",
    "#\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "1P58pxYkoN0j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feb 13th\n",
      "Feb 24th\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: using doc.ents, identify and print the dates in this sentence.\n",
    "# Expected output: ['Feb 13th', 'Feb 24th']\n",
    "#\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "s = \"We'll be in Osaka on Feb 13th and leave on Feb 24th.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Extract and print dates\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"DATE\":\n",
    "        print(ent.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "OVFi0bxCoN4N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 Caesar Augustus\n",
      "15 17 Roman Empire\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Read about spaCy's PhraseMatcher\n",
    "# https://spacy.io/usage/rule-based-matching#phrasematcher\n",
    "#\n",
    "# Using the PhraseMatcher, find the start and end index of all occurrences\n",
    "# of 'Caesar Augustus' and 'Roman Empire' (case-insensitive).\n",
    "#\n",
    "# Expected output: [(0, 2), (15, 17)]\n",
    "#\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    " \n",
    "s = \"Caesar Augustus was the founder of the Roman Principate (the first phase of the Roman Empire).\"\n",
    "\n",
    "patterns = [nlp.make_doc(name) for name in [\"Caesar Augustus\", \"Roman Empire\"]]\n",
    "matcher.add(\"Names\", patterns)\n",
    "\n",
    "doc = nlp(s)\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(start, end, doc[start:end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nhN7p9G8taJ"
   },
   "source": [
    "# Additional Reading and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM4G2KWa8wXO"
   },
   "source": [
    "Read through this page to learn more about spaCy's language processing pipeline including what's going on under the hood, how to create custom components, disable certain components (e.g. NER) when they're unneeded, optimization tips, and best practices:<br>\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "<br><br>\n",
    "Take the free and succinct spaCy course (available in multiple languages):<br>\n",
    "https://course.spacy.io/\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1FumrL3tYRZi0P7o-dBtmWmf2Hiv2KmL-",
     "timestamp": 1748435567175
    },
    {
     "file_id": "https://github.com/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_preprocessing.ipynb",
     "timestamp": 1748435063536
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
